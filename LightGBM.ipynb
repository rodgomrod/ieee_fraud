{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import gc\n",
    "import logging\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "from utils.schemas import *\n",
    "from utils.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_NAME = 'logs/LightGBM.log'\n",
    "logging.basicConfig(filename=LOG_NAME, level=logging.WARNING, format='%(asctime)s %(message)s')\n",
    "logging.warning(\"\")\n",
    "logging.warning(\"##### New LightGBM Model #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = pd.read_csv('docs/20190923_FeatureImportance_LGB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['D1','D10','D11','D12','D13','D14','D15','D2','D3','D4','D5','D6',\n",
    "#              'D7','D8','D9','C1','C10','C11','C12','C13','C14','C2','C3','C4','C5','C6','C7','C8','C9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cols = df_imp[df_imp.Importance >= 0].feature.to_list()\n",
    "X_cols = df_imp.feature[:500].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA_27',\n",
       " 'C1_div_C14',\n",
       " 'C11_div_C13',\n",
       " 'C13_div_C8',\n",
       " 'C1_div_C4',\n",
       " 'C1',\n",
       " 'C14_div_C4',\n",
       " 'C13_div_C2',\n",
       " 'V317']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cols[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readig train...\n",
      "Reduce_memory...\n",
      "Readig test...\n",
      "Reduce_memory...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv(data_folder+'/train_syn_ft_eng_1.csv', dtype = schema_synthetic_ft_eng_1, usecols=list(set(X_cols+['isFraud']+id_cols)))\n",
    "# test = pd.read_csv(data_folder+'/test_syn_ft_eng_1.csv', dtype = schema_synthetic_ft_eng_1, usecols=list(set(X_cols+['isFraud']+id_cols)))\n",
    "# train = pd.read_csv(data_folder+'/train_ft_eng_5.csv', usecols=X_cols+['isFraud', 'month'])\n",
    "# test = pd.read_csv(data_folder+'/test_ft_eng_5.csv', usecols=X_cols)\n",
    "\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "print('Readig train...')\n",
    "# train = pd.DataFrame()\n",
    "train = pd.read_csv(data_folder+'/train_ft_eng_7.csv.gz', usecols=X_cols+['isFraud','month'])\n",
    "# train = pd.concat([chunk, train], axis=1)\n",
    "dict_dtypes = reduce_memory2(train)\n",
    "train = train.astype(dict_dtypes)\n",
    "    \n",
    "print('Readig test...')\n",
    "# test = pd.DataFrame()\n",
    "test = pd.read_csv(data_folder+'/test_ft_eng_7.csv.gz', usecols=X_cols)\n",
    "#     test = pd.concat([chunk, test], axis=1)\n",
    "dict_dtypes = reduce_memory2(test)\n",
    "test = test.astype(dict_dtypes)\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_1 = train[train.isFraud == 1]\n",
    "# train_1.shape\n",
    "\n",
    "# train_0 = train[train.isFraud == 0].sample(train_1.shape[0]*3, random_state=42)\n",
    "\n",
    "# mini_train = pd.concat([train_1, train_0], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['day'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1))\n",
    "# test['day'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = train.groupby(['month']).grouper.group_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_dict = dict()\n",
    "for c in X_cols:\n",
    "    if c not in cat_ft:\n",
    "        imp_dict[c] = train[c].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = ['C1','C10','C11','C12','C13','C14','C2','C3','C4','C5','C6','C7',\n",
    "      'C8','C9','D1','D10','D11','D12','D13','D14','D15','D2','D3','D4','D5','D6','D7','D8',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['date',\n",
    " 'day','isFraud',\n",
    " 'month',\n",
    " 'year','date_fe1','date_fe2','TransactionDT','addr1',\n",
    " 'addr2',\n",
    " 'card_0',\n",
    " 'card_1',\n",
    " 'card_2',\n",
    " 'card_3',\n",
    " 'card_4',\n",
    " 'card_5',\n",
    " 'card_6',\n",
    " 'card_7',\n",
    " 'card_8',\n",
    " 'card_9',\n",
    " 'card_10',\n",
    " 'card_11',\n",
    " 'card_12',\n",
    " 'card_13',\n",
    " 'card_14',\n",
    " 'card_15',\n",
    " 'card1',\n",
    " 'card2',\n",
    " 'card3',\n",
    " 'card4',\n",
    " 'card5',\n",
    " 'card6','dist1','card1_fe1',\n",
    " 'card2_fe1',\n",
    " 'card3_fe1',\n",
    " 'card4_fe1',\n",
    " 'card5_fe1',\n",
    " 'card6_fe1',\n",
    " 'addr1_fe1',\n",
    " 'addr2_fe1','card_fe1','card1_fe2',\n",
    " 'card2_fe2',\n",
    " 'card3_fe2',\n",
    " 'card4_fe2',\n",
    " 'card5_fe2',\n",
    " 'card6_fe2',\n",
    " 'addr1_fe2',\n",
    " 'addr2_fe2','card_fe2','addr',] + dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = [x for x in train.columns if x not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cols = [x for x in X_cols if x not in ['day']+id_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_cols = [x for x in X_cols if x not in ['P_emaildomain_0', 'device_version_fe1', 'N17', 'N9', 'R_emaildomain_0', 'device_name_fe1', 'N7', 'R_emaildomain_1', 'N22', 'N10', 'N12', 'R_emaildomain_0_fe1', 'P_emaildomain_0_fe1', 'proc_id_30_0_fe1', 'N13', 'R_emaildomain_1_fe1', 'R_emaildomain_1_fe2', 'P_emaildomain_0_fe2', 'N21', 'proc_id_30_1_fe1', 'proc_id_31_0_fe1', 'proc_id_31_0_fe2', 'device_name', 'device_version']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_cols = [x for x in train.columns if 'M' in x or 'D' in x or 'C' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in new_cols:\n",
    "    train[c] = train[c] / train.groupby(['month'])[c].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[[x for x in train.columns if x not in ['isFraud','month']]]#.fillna(imp_dict)\n",
    "y = train.isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[[x for x in train.columns if x not in ['isFraud','month']]]\n",
    "# y_test = test.isFraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(417559, 501214)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_train_1 = train[(train.month < 4) | (train.month == 12)].shape[0]\n",
    "cut_train_2 = train[(train.month > 4) & (train.month != 12)].index[0]\n",
    "cut_train_1, cut_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarching best number rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fit = X[:cut_train_1]\n",
    "y_fit = y[:cut_train_1]\n",
    "\n",
    "X_val = X[cut_train_2:]\n",
    "y_val = y[cut_train_2:]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"Used columns: {}\".format(X_cols))\n",
    "k = 6\n",
    "logging.warning(\"Folds number: {}\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = X.index\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 191,\n",
    "    'max_depth': 15,\n",
    "    'max_leaf_nodes': 45,\n",
    "    'min_sample_leaf': 20,\n",
    "    'metric': ['AUC'],\n",
    "    'first_metric_only': True,\n",
    "    'n_estimators': 50000,\n",
    "    'num_threads': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'objective': 'xentropy',\n",
    "#     'n_jobs': -1,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 5,\n",
    "    'importance_type': 'gain',\n",
    "#     'lambda_l1': 0.05,\n",
    "#     'lambda_l2': 0.05,\n",
    "    'bagging_seed': 42,\n",
    "    'random_state':42,\n",
    "    'seed': 42,\n",
    "    'feature_fraction_seed': 42,\n",
    "    'drop_seed': 42,\n",
    "    'data_random_seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.891964\n",
      "[400]\tvalid_0's auc: 0.903942\n",
      "[600]\tvalid_0's auc: 0.908431\n",
      "[800]\tvalid_0's auc: 0.90962\n",
      "[1000]\tvalid_0's auc: 0.910258\n",
      "Early stopping, best iteration is:\n",
      "[932]\tvalid_0's auc: 0.910661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.7, bagging_freq=5, bagging_seed=42,\n",
       "               boosting_type='gbdt', class_weight=None, colsample_bytree=0.6,\n",
       "               data_random_seed=42, drop_seed=42, feature_fraction_seed=42,\n",
       "               first_metric_only=True, importance_type='gain',\n",
       "               learning_rate=0.01, max_depth=15, max_leaf_nodes=45,\n",
       "               metric=['AUC'], min_child_samples=20, min_child_weight=0.001,\n",
       "               min_sample_leaf=20, min_split_gain=0.0, n_estimators=50000,\n",
       "               n_jobs=-1, num_leaves=191, num_threads=-1, objective='xentropy',\n",
       "               random_state=42, reg_alpha=0.0, reg_lambda=0.0, seed=42,\n",
       "               silent=True, subsample=1.0, ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=200,\n",
    "                  early_stopping_rounds=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i = lgb_model.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_i = params['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['n_estimators'] = int(best_i*(5/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.7, bagging_freq=5, bagging_seed=42,\n",
       "               boosting_type='gbdt', class_weight=None, colsample_bytree=0.6,\n",
       "               data_random_seed=42, drop_seed=42, feature_fraction_seed=42,\n",
       "               first_metric_only=True, importance_type='gain',\n",
       "               learning_rate=0.01, max_depth=15, max_leaf_nodes=45,\n",
       "               metric=['AUC'], min_child_samples=20, min_child_weight=0.001,\n",
       "               min_sample_leaf=20, min_split_gain=0.0, n_estimators=1553,\n",
       "               n_jobs=-1, num_leaves=191, num_threads=-1, objective='xentropy',\n",
       "               random_state=42, reg_alpha=0.0, reg_lambda=0.0, seed=42,\n",
       "               silent=True, subsample=1.0, ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lgb_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(data_folder+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['isFraud'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>0.001645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0.001192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>0.002099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>0.003253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID   isFraud\n",
       "0        3663549  0.000424\n",
       "1        3663550  0.001645\n",
       "2        3663551  0.001192\n",
       "3        3663552  0.002099\n",
       "4        3663553  0.003253"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "D = today.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_name = '{0}_LightGBM_{1}'.format(D, round(mean_auc_score_gkf, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '{0}_LightGBM_best_i_5_3_ft_eng_7_500'.format(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190923_LightGBM_best_i_5_3_ft_eng_7_500'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submissions/{}.csv.gz'.format(submission_name), sep=',', header=True, index=None, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.warning(\"Searching best max_depth\")\n",
    "\n",
    "# for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "#     X_fit, X_val = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "#     y_fit, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# scores_dict = dict()\n",
    "# for i in range(5, 20):\n",
    "#     print('Max depth = {}'.format(i))\n",
    "#     logging.warning('Max depth = {}'.format(i))\n",
    "#     params['max_depth'] = i\n",
    "#     lgb_model = lgb.LGBMClassifier(**params)\n",
    "#     lgb_model.fit(X_fit,\n",
    "#                   y_fit,\n",
    "#                   eval_set=[(X_val, y_val), (X_fit, y_fit)],\n",
    "#                   verbose=100,\n",
    "#                   early_stopping_rounds=40)\n",
    "#     scores_dict[i] = lgb_model.best_score_['valid_0']['auc']\n",
    "\n",
    "# b = pd.DataFrame({'Depth': [x for x in scores_dict.keys()], 'AUC': [x for x in scores_dict.values()]})\n",
    "# md = b[b.AUC == b.AUC.max()].Depth\n",
    "# md.values[0]\n",
    "\n",
    "# params['max_depth'] = md.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_model = lgb.LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.warning(\"Params: {}\".format(str(lgb_model.get_params())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups_test = test.groupby(group_cols).grouper.group_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_strategy_skf = skf.split(train_ids, y)\n",
    "fold_strategy_gkf = group_kfold.split(X, y, train.month)\n",
    "fold_strategy_tss = tscv.split(X=X, y=y)\n",
    "\n",
    "# fold_strategy_test = group_kfold.split(X_test, y_test, groups_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(fold_strategy):\n",
    "    lgb_model = lgb.LGBMClassifier(**params)\n",
    "    logging.warning(\"Params: {}\".format(str(lgb_model.get_params())))\n",
    "    counter = 1\n",
    "    auc_score = 0\n",
    "    iterat = 0\n",
    "    list_iter = list()\n",
    "    y_preds = np.zeros(X_test.shape[0])\n",
    "    importances = np.zeros(X_test.shape[1])\n",
    "    \n",
    "    y_preds_normal = np.zeros(X_test.shape[0])\n",
    "\n",
    "    for train_index, test_index in fold_strategy:\n",
    "        print('Fold {}'.format(counter))\n",
    "        logging.warning(\"Training fold {}\".format(counter))\n",
    "\n",
    "        X_fit, X_val = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_fit, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        lgb_model.fit(X_fit,\n",
    "                      y_fit,\n",
    "    #                   eval_set=[(X_val, y_val), (X_fit, y_fit)],\n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      verbose=200,\n",
    "                      early_stopping_rounds=40)\n",
    "\n",
    "        logging.warning(\"Best AUC in this fold: {}\".format(lgb_model.best_score_['valid_0']['auc']))\n",
    "        logging.warning(\"Best iteration in this fold: {}\".format(lgb_model.best_iteration_))\n",
    "        auc_score += lgb_model.best_score_['valid_0']['auc']\n",
    "        it = lgb_model.best_iteration_\n",
    "        iterat += it\n",
    "        list_iter.append(it)\n",
    "        importances += lgb_model.feature_importances_/k\n",
    "        predictions = lgb_model.predict_proba(X_test)[:,1]\n",
    "        y_preds_normal += predictions\n",
    "        predictions = np.array([x[0] for x in scaler.fit_transform(predictions.reshape(-1, 1))])\n",
    "        \n",
    "#         print(predictions)\n",
    "        y_preds += predictions/k\n",
    "\n",
    "\n",
    "        del X_fit\n",
    "        del X_val\n",
    "        del y_fit\n",
    "        del y_val\n",
    "        del train_index\n",
    "        del test_index\n",
    "        gc.collect()\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    mean_auc_score = auc_score/k\n",
    "    mean_iterat = iterat/k\n",
    "\n",
    "    logging.warning(\"Mean AUC in {0} folds: {1}\".format(k, mean_auc_score))\n",
    "    logging.warning(\"Mean iterations in {0} folds: {1}\".format(k, mean_iterat))\n",
    "    \n",
    "    return y_preds, y_preds_normal, importances, mean_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training with GoupKFold')\n",
    "y_preds_gkf, y_preds_normal_gkf, importances_gkf, mean_auc_score_gkf = make_predictions(fold_strategy_gkf)\n",
    "print('')\n",
    "print('Training with StratifiedFold')\n",
    "y_preds_skf, y_preds_normal_skf, importances_skf, mean_auc_score_skf = make_predictions(fold_strategy_skf)\n",
    "print('')\n",
    "# print('Training with TimeSeriesSplit')\n",
    "# y_preds_tss, importances_tss, mean_auc_score_tss = make_predictions(fold_strategy_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_gkf[230:235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame({'feature': X.columns, 'importance': importances_gkf})\\\n",
    ".sort_values('importance', ascending = False)\n",
    "df_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_final = (y_preds_gkf + y_preds_tss) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(data_folder+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['isFraud'] = y_preds_gkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "D = today.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '{0}_LightGBM_{1}'.format(D, round(mean_auc_score_gkf, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"Submission name: {}\".format(submission_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submissions/{}.csv'.format(submission_name), sep=',', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.to_csv('docs/ft_importances_{}.csv'.format(D), index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "auc_score = 0\n",
    "iterat = 0\n",
    "list_iter = list()\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "importances = np.zeros(X_test.shape[1])\n",
    "\n",
    "for train_index, test_index in fold_strategy:\n",
    "    print('Fold {}'.format(counter))\n",
    "    logging.warning(\"Training fold {}\".format(counter))\n",
    "\n",
    "    X_fit, X_val = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_fit, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "#                   eval_set=[(X_val, y_val), (X_fit, y_fit)],\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=200,\n",
    "                  early_stopping_rounds=40)\n",
    "    \n",
    "    logging.warning(\"Best AUC in this fold: {}\".format(lgb_model.best_score_['valid_0']['auc']))\n",
    "    logging.warning(\"Best iteration in this fold: {}\".format(lgb_model.best_iteration_))\n",
    "    auc_score += lgb_model.best_score_['valid_0']['auc']\n",
    "#     lgb_model.predict_proba(test[X.columns])\n",
    "#     preds = lgb_model.predict_proba(X_test[X.columns])[:,1]\n",
    "#     preds = lgb_model.predict_proba(X_fit_test[X.columns])[:,1]\n",
    "#     print('AUC test score: {}\\n'.format(roc_auc_score(y_fit_test, preds)))\n",
    "#     print('AUC 20% test score: {}'.format(roc_auc_score(y_test[:int(len(y_test)*0.2)],\n",
    "#                                                         preds[:int(len(y_test)*0.2)])))\n",
    "#     print('AUC 80% test score: {}'.format(roc_auc_score(y_test[int(len(y_test)*0.2):],\n",
    "#                                                         preds[int(len(y_test)*0.2):])))\n",
    "    it = lgb_model.best_iteration_\n",
    "    iterat += it\n",
    "    list_iter.append(it)\n",
    "    importances += lgb_model.feature_importances_/k\n",
    "    y_preds += lgb_model.predict_proba(X_test)[:,1]/k\n",
    "\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "mean_auc_score = auc_score/k\n",
    "mean_iterat = iterat/k\n",
    "\n",
    "logging.warning(\"Mean AUC in {0} folds: {1}\".format(k, mean_auc_score))\n",
    "logging.warning(\"Mean iterations in {0} folds: {1}\".format(k, mean_iterat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_importaces = pd.DataFrame({'feature': X.columns, 'importance': importances})\\\n",
    ".sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "# df_importaces['cs'] = df_importaces.importance.cumsum()\n",
    "# # df_importaces.cs = df_importaces.cs/df_importaces.cs.max()\n",
    "# df_importaces.importance = df_importaces.importance/df_imp.cs\n",
    "df_importaces.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(data_folder+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['isFraud'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "D = today.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '{0}_LightGBM_{1}'.format(D, round(mean_auc_score, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"Submission name: {}\".format(submission_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submissions/{}.csv'.format(submission_name), sep=',', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groups = new_X.groupby(group_cols).grouper.group_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "auc_score = 0\n",
    "iterat = 0\n",
    "list_iter = list()\n",
    "# for train_index, test_index in skf.split(train_ids, y):\n",
    "for train_index, test_index in group_kfold.split(new_X, new_y, new_groups):\n",
    "# for train_index, test_index in tscv.split(X=X, y=y):\n",
    "    print('Fold {}\\n'.format(counter))\n",
    "    logging.warning(\"Training fold {}\".format(counter))\n",
    "\n",
    "    X_fit, X_val = new_X.iloc[train_index, :], new_X.iloc[test_index, :]\n",
    "    y_fit, y_val = new_y.iloc[train_index], new_y.iloc[test_index]\n",
    "\n",
    "    lgb_model.fit(X_fit,\n",
    "                  y_fit,\n",
    "                  eval_set=[(X_val, y_val), (X_fit, y_fit)],\n",
    "                  verbose=100,\n",
    "                  early_stopping_rounds=100)\n",
    "    \n",
    "    logging.warning(\"Best AUC in this fold: {}\".format(lgb_model.best_score_['valid_0']['auc']))\n",
    "    logging.warning(\"Best iteration in this fold: {}\".format(lgb_model.best_iteration_))\n",
    "    auc_score += lgb_model.best_score_['valid_0']['auc']\n",
    "    preds = lgb_model.predict_proba(X_test[X.columns])[:,1]\n",
    "    print('AUC test score: {}'.format(roc_auc_score(y_test, preds)))\n",
    "    print('AUC 20% test score: {}'.format(roc_auc_score(y_test[:int(len(y_test)*0.2)],\n",
    "                                                        preds[:int(len(y_test)*0.2)])))\n",
    "    print('AUC 80% test score: {}'.format(roc_auc_score(y_test[int(len(y_test)*0.2):],\n",
    "                                                        preds[int(len(y_test)*0.2):])))\n",
    "    it = lgb_model.best_iteration_\n",
    "    iterat += it\n",
    "    list_iter.append(it)\n",
    "\n",
    "    del X_fit\n",
    "    del X_val\n",
    "    del y_fit\n",
    "    del y_val\n",
    "    del train_index\n",
    "    del test_index\n",
    "    gc.collect()\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    break\n",
    "    \n",
    "mean_auc_score = auc_score/k\n",
    "mean_iterat = iterat/k\n",
    "\n",
    "logging.warning(\"Mean AUC in {0} folds: {1}\".format(k, mean_auc_score))\n",
    "logging.warning(\"Mean iterations in {0} folds: {1}\".format(k, mean_iterat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iterat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['n_estimators'] = int(mean_iterat)\n",
    "params['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "lgb_model.fit(train[X_cols+drop_cols], train.isFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = lgb_model.predict_proba(test[X.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(data_folder+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['isFraud'] = y_preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "D = today.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = '{0}_LightGBM_{1}'.format(D, mean_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"Submission name: {}\".format(submission_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submissions/{}.csv'.format(submission_name), sep=',', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.warning(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[df_sub.isFraud>0.9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
